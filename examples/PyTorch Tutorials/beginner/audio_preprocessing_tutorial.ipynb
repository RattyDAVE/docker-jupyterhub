{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install torch>=1.2.0\n",
        "!pip install torchaudio\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ntorchaudio Tutorial\n===================\n\nPyTorch is an open source deep learning platform that provides a\nseamless path from research prototyping to production deployment with\nGPU support.\n\nSignificant effort in solving machine learning problems goes into data\npreparation. torchaudio leverages PyTorch\u2019s GPU support, and provides\nmany tools to make data loading easy and more readable. In this\ntutorial, we will see how to load and preprocess data from a simple\ndataset.\n\nFor this tutorial, please make sure the ``matplotlib`` package is\ninstalled for easier visualization.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchaudio\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Opening a dataset\n-----------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "torchaudio supports loading sound files in the wav and mp3 format. We\ncall waveform the resulting raw audio signal.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
    "import requests\n",
    "\n",
    "url = \"https://pytorch.org/tutorials//_static/img/steam-train-whistle-daniel_simon-converted-from-mp3.wav\"\n",
    "r = requests.get(url)\n",
    "\n",
    "with open('steam-train-whistle-daniel_simon-converted-from-mp3.wav', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "filename = \"steam-train-whistle-daniel_simon-converted-from-mp3.wav\"\n",
    "waveform, sample_rate = torchaudio.load(filename)\n",
    "\n",
    "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
    "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(waveform.t().numpy())"      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transformations\n---------------\n\ntorchaudio supports a growing list of\n`transformations <https://pytorch.org/audio/transforms.html>`_.\n\n-  **Resample**: Resample waveform to a different sample rate.\n-  **Spectrogram**: Create a spectrogram from a waveform.\n-  **MelScale**: This turns a normal STFT into a Mel-frequency STFT,\n   using a conversion matrix.\n-  **AmplitudeToDB**: This turns a spectrogram from the\n   power/amplitude scale to the decibel scale.\n-  **MFCC**: Create the Mel-frequency cepstrum coefficients from a\n   waveform.\n-  **MelSpectrogram**: Create MEL Spectrograms from a waveform using the\n   STFT function in PyTorch.\n-  **MuLawEncoding**: Encode waveform based on mu-law companding.\n-  **MuLawDecoding**: Decode mu-law encoded waveform.\n\nSince all transforms are nn.Modules or jit.ScriptModules, they can be\nused as part of a neural network at any point.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To start, we can look at the log of the spectrogram on a log scale.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "specgram = torchaudio.transforms.Spectrogram()(waveform)\n\nprint(\"Shape of spectrogram: {}\".format(specgram.size()))\n\nplt.figure()\nplt.imshow(specgram.log2()[0,:,:].numpy(), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or we can look at the Mel Spectrogram on a log scale.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "specgram = torchaudio.transforms.MelSpectrogram()(waveform)\n\nprint(\"Shape of spectrogram: {}\".format(specgram.size()))\n\nplt.figure()\np = plt.imshow(specgram.log2()[0,:,:].detach().numpy(), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can resample the waveform, one channel at a time.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_sample_rate = sample_rate/10\n\n# Since Resample applies to a single channel, we resample first channel here\nchannel = 0\ntransformed = torchaudio.transforms.Resample(sample_rate, new_sample_rate)(waveform[channel,:].view(1,-1))\n\nprint(\"Shape of transformed waveform: {}\".format(transformed.size()))\n\nplt.figure()\nplt.plot(transformed[0,:].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As another example of transformations, we can encode the signal based on\nMu-Law enconding. But to do so, we need the signal to be between -1 and\n1. Since the tensor is just a regular PyTorch tensor, we can apply\nstandard operators on it.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Let's check if the tensor is in the interval [-1,1]\nprint(\"Min of waveform: {}\\nMax of waveform: {}\\nMean of waveform: {}\".format(waveform.min(), waveform.max(), waveform.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the waveform is already between -1 and 1, we do not need to\nnormalize it.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def normalize(tensor):\n    # Subtract the mean, and scale to the interval [-1,1]\n    tensor_minusmean = tensor - tensor.mean()\n    return tensor_minusmean/tensor_minusmean.abs().max()\n\n# Let's normalize to the full interval [-1,1]\n# waveform = normalize(waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u2019s apply encode the waveform.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transformed = torchaudio.transforms.MuLawEncoding()(waveform)\n\nprint(\"Shape of transformed waveform: {}\".format(transformed.size()))\n\nplt.figure()\nplt.plot(transformed[0,:].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now decode.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reconstructed = torchaudio.transforms.MuLawDecoding()(transformed)\n\nprint(\"Shape of recovered waveform: {}\".format(reconstructed.size()))\n\nplt.figure()\nplt.plot(reconstructed[0,:].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can finally compare the original waveform with its reconstructed\nversion.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute median relative difference\nerr = ((waveform-reconstructed).abs() / waveform.abs()).median()\n\nprint(\"Median relative difference between original and MuLaw reconstucted signals: {:.2%}\".format(err))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Migrating to torchaudio from Kaldi\n----------------------------------\n\nUsers may be familiar with\n`Kaldi <http://github.com/kaldi-asr/kaldi>`_, a toolkit for speech\nrecognition. torchaudio offers compatibility with it in\n``torchaudio.kaldi_io``. It can indeed read from kaldi scp, or ark file\nor streams with:\n\n-  read_vec_int_ark\n-  read_vec_flt_scp\n-  read_vec_flt_arkfile/stream\n-  read_mat_scp\n-  read_mat_ark\n\ntorchaudio provides Kaldi-compatible transforms for ``spectrogram`` and\n``fbank`` with the benefit of GPU support, see\n`here <compliance.kaldi.html>`__ for more information.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_fft = 400.0\nframe_length = n_fft / sample_rate * 1000.0\nframe_shift = frame_length / 2.0\n\nparams = {\n    \"channel\": 0,\n    \"dither\": 0.0,\n    \"window_type\": \"hanning\",\n    \"frame_length\": frame_length,\n    \"frame_shift\": frame_shift,\n    \"remove_dc_offset\": False,\n    \"round_to_power_of_two\": False,\n    \"sample_frequency\": sample_rate,\n}\n\nspecgram = torchaudio.compliance.kaldi.spectrogram(waveform, **params)\n\nprint(\"Shape of spectrogram: {}\".format(specgram.size()))\n\nplt.figure()\nplt.imshow(specgram.t().numpy(), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also support computing the filterbank features from waveforms,\nmatching Kaldi\u2019s implementation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fbank = torchaudio.compliance.kaldi.fbank(waveform, **params)\n\nprint(\"Shape of fbank: {}\".format(fbank.size()))\n\nplt.figure()\nplt.imshow(fbank.t().numpy(), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n----------\n\nWe used an example raw audio signal, or waveform, to illustrate how to\nopen an audio file using torchaudio, and how to pre-process and\ntransform such waveform. Given that torchaudio is built on PyTorch,\nthese techniques can be used as building blocks for more advanced audio\napplications, such as speech recognition, while leveraging GPUs.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

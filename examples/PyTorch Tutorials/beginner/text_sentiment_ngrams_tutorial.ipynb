{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install torch<=1.2.0\n",
        "!pip install torchtext\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nText Classification Tutorial\n============================\n\nThis tutorial shows how to use the text classification datasets,\nincluding\n\n::\n\n   - AG_NEWS,\n   - SogouNews, \n   - DBpedia, \n   - YelpReviewPolarity,\n   - YelpReviewFull, \n   - YahooAnswers, \n   - AmazonReviewPolarity,\n   - AmazonReviewFull\n\nThis example shows the application of ``TextClassification`` Dataset for\nsupervised learning analysis.\n\nLoad data with ngrams\n---------------------\n\nA bag of ngrams feature is applied to capture some partial information\nabout the local word order. In practice, bi-gram or tri-gram are applied\nto provide more benefits as word groups than only one word. An example:\n\n::\n\n   \"load data with ngrams\"\n   Bi-grams results: \"load data\", \"data with\", \"with ngrams\"\n   Tri-grams results: \"load data with\", \"data with ngrams\"\n\n``TextClassification`` Dataset supports the ngrams method. By setting\nngrams to 2, the example text in the dataset will be a list of single\nwords plus bi-grams string.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchtext\nfrom torchtext.datasets import text_classification\nNGRAMS = 2\nimport os\nif not os.path.isdir('./.data'):\n\tos.mkdir('./.data')\ntrain_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n    root='./.data', ngrams=NGRAMS, vocab=None)\nBATCH_SIZE = 16\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the model\n----------------\n\nThe model is composed of the\n`EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>`__\nlayer and the linear layer (see the figure below). ``nn.EmbeddingBag``\ncomputes the mean value of a \u201cbag\u201d of embeddings. The text entries here\nhave different lengths. ``nn.EmbeddingBag`` requires no padding here\nsince the text lengths are saved in offsets.\n\nAdditionally, since ``nn.EmbeddingBag`` accumulates the average across\nthe embeddings on the fly, ``nn.EmbeddingBag`` can enhance the\nperformance and memory efficiency to process a sequence of tensors.\n\n![](../_static/img/text_sentiment_ngrams_model.png)\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\nimport torch.nn.functional as F\nclass TextSentiment(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.fc = nn.Linear(embed_dim, num_class)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n        \n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initiate an instance\n--------------------\n\nThe AG_NEWS dataset has four labels and therefore the number of classes\nis four.\n\n::\n\n   1 : World\n   2 : Sports\n   3 : Business\n   4 : Sci/Tec\n\nThe vocab size is equal to the length of vocab (including single word\nand ngrams). The number of classes is equal to the number of labels,\nwhich is four in AG_NEWS case.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\nEMBED_DIM = 32\nNUN_CLASS = len(train_dataset.get_labels())\nmodel = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions used to generate batch\n--------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the text entries have different lengths, a custom function\ngenerate_batch() is used to generate data batches and offsets. The\nfunction is passed to ``collate_fn`` in ``torch.utils.data.DataLoader``.\nThe input to ``collate_fn`` is a list of tensors with the size of\nbatch_size, and the ``collate_fn`` function packs them into a\nmini-batch. Pay attention here and make sure that ``collate_fn`` is\ndeclared as a top level def. This ensures that the function is available\nin each worker.\n\nThe text entries in the original data batch input are packed into a list\nand concatenated as a single tensor as the input of ``nn.EmbeddingBag``.\nThe offsets is a tensor of delimiters to represent the beginning index\nof the individual sequence in the text tensor. Label is a tensor saving\nthe labels of individual text entries.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_batch(batch):\n    label = torch.tensor([entry[0] for entry in batch])\n    text = [entry[1] for entry in batch]\n    offsets = [0] + [len(entry) for entry in text]\n    # torch.Tensor.cumsum returns the cumulative sum\n    # of elements in the dimension dim.\n    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n    \n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    text = torch.cat(text)\n    return text, offsets, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define functions to train the model and evaluate results.\n---------------------------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__\nis recommended for PyTorch users, and it makes data loading in parallel\neasily (a tutorial is\n`here <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`__).\nWe use ``DataLoader`` here to load AG_NEWS datasets and send it to the\nmodel for training/validation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n\ndef train_func(sub_train_):\n\n    # Train the model\n    train_loss = 0\n    train_acc = 0\n    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n                      collate_fn=generate_batch)\n    for i, (text, offsets, cls) in enumerate(data):\n        optimizer.zero_grad()\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        output = model(text, offsets)\n        loss = criterion(output, cls)\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        train_acc += (output.argmax(1) == cls).sum().item()\n\n    # Adjust the learning rate\n    scheduler.step()\n    \n    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n\ndef test(data_):\n    loss = 0\n    acc = 0\n    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n    for text, offsets, cls in data:\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        with torch.no_grad():\n            output = model(text, offsets)\n            loss = criterion(output, cls)\n            loss += loss.item()\n            acc += (output.argmax(1) == cls).sum().item()\n\n    return loss / len(data_), acc / len(data_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split the dataset and run the model\n-----------------------------------\n\nSince the original AG_NEWS has no valid dataset, we split the training\ndataset into train/valid sets with a split ratio of 0.95 (train) and\n0.05 (valid). Here we use\n`torch.utils.data.dataset.random_split <https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split>`__\nfunction in PyTorch core library.\n\n`CrossEntropyLoss <https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\ncriterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.\nIt is useful when training a classification problem with C classes.\n`SGD <https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html>`__\nimplements stochastic gradient descent method as optimizer. The initial\nlearning rate is set to 4.0.\n`StepLR <https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR>`__\nis used here to adjust the learning rate through epochs.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nfrom torch.utils.data.dataset import random_split\nN_EPOCHS = 5\nmin_valid_loss = float('inf')\n\ncriterion = torch.nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=4.0)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n\ntrain_len = int(len(train_dataset) * 0.95)\nsub_train_, sub_valid_ = \\\n    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    train_loss, train_acc = train_func(sub_train_)\n    valid_loss, valid_acc = test(sub_valid_)\n\n    secs = int(time.time() - start_time)\n    mins = secs / 60\n    secs = secs % 60\n\n    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the model on GPU with the following information:\n\nEpoch: 1 \\| time in 0 minutes, 11 seconds\n\n::\n\n       Loss: 0.0263(train)     |       Acc: 84.5%(train)\n       Loss: 0.0001(valid)     |       Acc: 89.0%(valid)\n\n\nEpoch: 2 \\| time in 0 minutes, 10 seconds\n\n::\n\n       Loss: 0.0119(train)     |       Acc: 93.6%(train)\n       Loss: 0.0000(valid)     |       Acc: 89.6%(valid)\n\n\nEpoch: 3 \\| time in 0 minutes, 9 seconds\n\n::\n\n       Loss: 0.0069(train)     |       Acc: 96.4%(train)\n       Loss: 0.0000(valid)     |       Acc: 90.5%(valid)\n\n\nEpoch: 4 \\| time in 0 minutes, 11 seconds\n\n::\n\n       Loss: 0.0038(train)     |       Acc: 98.2%(train)\n       Loss: 0.0000(valid)     |       Acc: 90.4%(valid)\n\n\nEpoch: 5 \\| time in 0 minutes, 11 seconds\n\n::\n\n       Loss: 0.0022(train)     |       Acc: 99.0%(train)\n       Loss: 0.0000(valid)     |       Acc: 91.0%(valid)        \n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the model with test dataset\n------------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Checking the results of test dataset...')\ntest_loss, test_acc = test(test_dataset)\nprint(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checking the results of test dataset\u2026\n\n::\n\n       Loss: 0.0237(test)      |       Acc: 90.5%(test)\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test on a random news\n---------------------\n\nUse the best model so far and test a golf news. The label information is\navailable\n`here <https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS>`__.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import re\nfrom torchtext.data.utils import ngrams_iterator\nfrom torchtext.data.utils import get_tokenizer\n\nag_news_label = {1 : \"World\",\n                 2 : \"Sports\",\n                 3 : \"Business\",\n                 4 : \"Sci/Tec\"}\n\ndef predict(text, model, vocab, ngrams):\n    tokenizer = get_tokenizer(\"basic_english\")\n    with torch.no_grad():\n        text = torch.tensor([vocab[token]\n                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n        output = model(text, torch.tensor([0]))\n        return output.argmax(1).item() + 1\n\nex_text_str = \"MEMPHIS, Tenn. \u2013 Four days ago, Jon Rahm was \\\n    enduring the season\u2019s worst weather conditions on Sunday at The \\\n    Open on his way to a closing 75 at Royal Portrush, which \\\n    considering the wind and the rain was a respectable showing. \\\n    Thursday\u2019s first round at the WGC-FedEx St. Jude Invitational \\\n    was another story. With temperatures in the mid-80s and hardly any \\\n    wind, the Spaniard was 13 strokes better in a flawless round. \\\n    Thanks to his best putting performance on the PGA Tour, Rahm \\\n    finished with an 8-under 62 for a three-stroke lead, which \\\n    was even more impressive considering he\u2019d never played the \\\n    front nine at TPC Southwind.\"\n\nvocab = train_dataset.get_vocab()\nmodel = model.to(\"cpu\")\n\nprint(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a Sports news\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can find the code examples displayed in this note\n`here <https://github.com/pytorch/text/tree/master/examples/text_classification>`__.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
